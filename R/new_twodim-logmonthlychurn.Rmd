---
title: "Churn: Two dimensions and predict log of monthly churn probability"
output: html_notebook
---

```{r setup, include=FALSE}
source('config.R')
source('utils.R')
source('utils_validation.R')

# source('new_lifetimepredictor.R')
```

Read prepared data.

```{r reading, tidy=F}
subscriptions <- read_rds('../data/subscriptions.rds')
```

```{r}
summary(subscriptions)
```

```{r}
subscriptions_with_target <- subscriptions %>%
  # restrict to a recent expiry window
  filter(endmonth >= begin_train_window & endmonth < end_window) %>%
  mutate(num_previous_months_binned_fct = as.factor(num_previous_months_binned)) %>%
  mutate(set_type = as.factor(if_else(endmonth >= begin_validation_window, 'validation', 'training'))) %>%
  
  mutate(churnind = ifelse(status == 'churn', 1, 0)) 
```


Prepare churntable that we want to predict.

```{r}
churntable <- subscriptions_with_target %>%
  
  group_by(set_type, siteverkey_cat2, market_category, months, num_previous_months_binned, chosen_subs_length, subscription_summary_no_market) %>%
  summarise(num_obs = n(), 
            churned = sum(churnind)) %>%
  
  group_by(set_type) %>%
  mutate(churn_rate = churned / num_obs,
         renew_rate = 1 - churn_rate,
         month_churn = 1 - renew_rate ^ (1/as.double(months)),
         log_month_churn = log(month_churn),
         weight = num_obs / sum(num_obs))

# NB! Does this introduce a bad bias ????
churntable_no_zeros <- churntable %>%
  filter(churn_rate > 0)
```

Train model

```{r}
new_model=glm(log_month_churn ~ market_category + subscription_summary_no_market, data=churntable_no_zeros[churntable_no_zeros$set_type == 'training', ], weights = weight)
```

Model validation for training (2017-01-01 - 2017-08-01) and validation (2017-09-01 - 2018-01-01) sets:

* Summary table containing
    + Number of observations without prediction
    + AUC, logloss - prediction quality metrics
* ROC curve
* Plots per market
    + Age of customers vs real and predicted probability of churn for different subscription lengths. It shows if we are correctly predicting probability of churn for customers over lifetime.
    + Calibration - Predicted probability of churn vs real probability of churn for different subscription lengths (well calibrated prediction should form a diagonal line). Shows if outcome of model in question can be realy treated as probability.

```{r warning=F}
prediction_table <- validation(subscriptions_with_target, new_model, predict_2fct_model)
```

```{r fig.height=10, fig.width=7, warning=F}
validation_plots(prediction_table, minimal_share = 0.01)
```

Try simple logistic model
```{r}
model_logit <- glm(churnind ~ market_category + siteverkey_cat2 + num_previous_months_binned + months + chosen_subs_length,
                   data = subscriptions_with_target[subscriptions_with_target$set_type == 'training',], family = 'binomial')
```


Model validation for training (2017-01-01 - 2017-08-01) and validation (2017-09-01 - 2018-01-01) sets:

* Summary table containing
    + Number of observations without prediction
    + AUC, logloss - prediction quality metrics
* ROC curve
* Plots per market
    + Age of customers vs real and predicted probability of churn for different subscription lengths. It shows if we are correctly predicting probability of churn for customers over lifetime.
    + Calibration - Predicted probability of churn vs real probability of churn for different subscription lengths (well calibrated prediction should form a diagonal line). Shows if outcome of model in question can be realy treated as probability.
```{r warning=F}
prediction_table_logit <- validation(subscriptions_with_target, model_logit)
```

```{r fig.height=10, fig.width=7, warning=F}
validation_plots(prediction_table_logit, minimal_share = 0.01)
```